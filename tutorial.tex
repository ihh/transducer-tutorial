\documentclass{article}

\newcommand{\secref}[1]{Subsection~\ref{sec.#1}}
\newcommand{\seclabel}[1]{\label{sec.#1}}

\newcommand{\figref}[1]{Figure~\ref{Figures.#1}}
\newcommand{\figlabel}[1]{\label{Figures.#1}}

\newcommand{\needfig}[1]{{\bf Need figure: } #1 }
\newcommand{\needfigref}[1]{Figure~??? [#1] }

\newcommand{\needcite}[1]{[CITE #1]}
\newcommand{\todo}[1]{[TODO: #1]}

\newcommand{\easyfig}[4]{
\begin{figure}
\includegraphics[#2]{#1}
\caption{ \figlabel{#3} #4}
\end{figure}}

\newcommand{\pngfig}[2]{\easyfig{#1.png}{}{#1}{#2}}
\newcommand{\pdffig}[2]{\easyfig{#1-fig.pdf}{}{#1}{#2}}

\newcommand{\widepngfig}[2]{\easyfig{#1.png}{width=\textwidth}{#1}{#2}}
\newcommand{\widepdffig}[2]{\easyfig{#1-fig.pdf}{width=\textwidth}{#1}{#2}}
\newcommand{\tallpdffig}[2]{\easyfig{#1-fig.pdf}{height=\textheight}{#1}{#2}}

\usepackage{graphicx}
\usepackage{url}

\begin{document}
\title{Transducer Tutorial}
\date{}
\maketitle

\section{Tutorial overview}

Our algorithm may be viewed as a consolidation of several approaches:
\begin{itemize}
\item Felsenstein's pruning algorithm for calculating statistical phylogenetic likelihoods
\item Sankoff's ${\cal O}(L^N)$ algorithm for simultaneous multiple alignment, ancestral reconstruction and phylogeny
\item Heuristic approximations (at ${\cal O}(NL^2)$ or thereabouts) to Sankoff's algorithm including progressive alignment and partial-order graph alignment
\item Hidden Markov Model theory for sequence profiling
\item The Thorne-Kishino-Felsenstein (1991) model for indels and the related algorithm of Hein (2001)
\item Algorithms for ancestral sequence reconstruction conditioned on an alignment (TreeHMM, Indelign, Ortheus)
\end{itemize}

The algorithm uses the theory of finite state transducers to connect these fields,
and suggests a way to extend the literature on
site-dependent and latent-variable substitution models \needcite{Yang,Goldman,Bruno,Holmes...}
to indel models.

\subsection{Transducers as input-output machines}

We begin with a brief definition of transducers from Wikipedia...

% Begin quote from Wikipedia
{\em A finite state transducer is a finite state machine with two tapes: an input tape and an output tape. ... An automaton can be said to } recognize {\em a string if we view the content of its tape as input. In other words, the automaton computes a function that maps strings into the set $\{0,1\}$. $(\dagger\dagger\dagger)$ Alternatively, we can say that an automaton } generates {\em strings, which means viewing its tape as an output tape. On this view, the automaton generates a formal language, which is a set of strings. The two views of automata are equivalent: the function that the automaton computes is precisely the indicator function of the set of strings it generates... Finite State Transducers can be weighted, where each transition is labeled with a weight in addition to the input and output labels. }
\url{http://en.wikipedia.org/wiki/Finite_state_transducer}
% End quote from Wikipedia
\\
$(\dagger\dagger\dagger)$ For a weighted transducer this mapping is,
more generally, to the positive real axis $[0,\infty)$
rather than just the binary set $\{0,1\}$.

In this tutorial we are going to work through some small examples of using transducers on trees,
for some tiny example protein sequences (MF, CS, LIV).
Forbled example we will compute the likelihood of the symmetric binary tree shown in \figref{cs-mf-liv-tree},
explaining the common descent of these three sequences
under the TKF91 model.
\pdffig{cs-mf-liv-tree}{Example tree used in this tutorial.}

As noted in the Wikipedia quote, transducers can be thought of as generalizations of the related
concepts of {\em generators} (state machines that emit output sequences, such as HMMs)
and parsers or {\em recognizers} (state machines that match/parse input sequences, such as the UNIX 'lex' program).
Both generators and recognizers are separate special cases of transducers.
Of particular use in our treatment are generators/recognizers that generate/recognize a single unique sequence.
\figref{mf-generator} is an example of a generator that uniquely generates the protein sequence MF;
we write this as $\nabla(MF)$.
\figref{liv-small} is an example of a recognizer that uniquely recognizes the protein sequence LIV;
we write this as $\Delta(LIV)$.
\todo{Change generators to $\Delta$ and recognizers to $\nabla$;
 implement this universally in figures, formal definitions, and tutorial}

\pngfig{mf-generator}{Generator for protein sequence MF.}

\pngfig{liv-small}{Recognizer for protein sequence LIV.}

These figures illustrate the notation we use in this tutorial.
States and transitions are shown as a graph.
Transitions can be labeled with absorption/emission pairs,
written $x/y$ where $x$ is the absorbed character and $y$ the emitted character.
Either $x$ or $y$ is allowed to be the empty string (shown in these diagrams as the gap character, a hyphen).
In a figure that shows absorption/emission pairs,
if there is no absorption/emission labeled on a transition, then it can be assumed to be $-/-$
(i.e. no character is absorbed or emitted) and the transition is said to be a ``null'' transition.

Some transitions are also labeled with weights.
If no transition label is present, the weight is usually 1
(some more complicated diagrams omit all the weights, to avoid clutter).

Weight of a path is product of transition weights...

Weight of an input-output sequence pair is sum over all path weights that generate those input and output sequences...

Note sometimes this weight is zero --- e.g. in \figref{liv-small} the weight is zero
except in the unique case that the input tape is LIV, when the weight is one ---
this in fact makes \figref{liv-small} a special kind of recognizer:
one that only recognizes a single string
(and recognizes that string with probability one).
We call this an {\em exact-match} recognizer.

More generally, suppose that $G$ and $R$ are both probabilistically weighted finite-state transducers,
but $G$ is a generator and $R$ is a recognizer.
Then, conventionally, $G$ defines a probability $P(Y|G)$ of emitting any emitted output sequence $Y$,
while $R$ defines a probability $P(\mbox{recognized}|X,R)$ of accepting any input sequence $R$.
Note that while the former, $P(Y|G)$, has an interpretation as a probability distribution over $Y$
(so it's reasonable to expect the sum over $Y$ to be one),
the latter, $P(\mbox{recognized}|X,R)$, is conditional on $X$
(in general not reasonable to expect sum over $X$ to be one,
although it is for the special case of an exact-match recognizer).
{\bf It is important to state that these are just conventional interpretations of the computed weights:}
in principle the weights can mean anything we want,
but it is common to interpret them as probabilities in this way.

Thus, as noted in the Wikipedia quote, generators and recognizers are in some sense equivalent,
although the probabilistic interpretations of the weights are slightly different.
In particular, just as we can have a {\em generative profile}
that generates some sequences with higher probability than others (e.g. a profile HMM)
we can also have a {\em recognition profile}: a transducer
that recognizes some sequences with higher probability than others.
The exact-match transducer of \figref{liv-small} is a (trivial and extreme) example of such a recognizer;
later we will see that the conditional probabilities in the Felsenstein pruning recursion can also
be thought of as recognition profiles.

\subsection{Moore machines}

Mealy machines: input/output associated with transitions.
This is the way we define things mathematically...

Moore machines: input/output associated with states.
Can be more useful in bioinformatics,
where point substitution means that all combinations of input and output characters are frequently observed...

For example consider the Mealy-machine-like view of \figref{fanned-emission}
\widepngfig{fanned-emission}{All combinations of input and output characters are frequently observed...}

It can be more convenient to represent this as a Moore-machine-like view of \figref{condensed-emission}
\widepngfig{condensed-emission}{Condensed representation of all combinations of input and output characters...}

Note the features of this view:
\begin{itemize}
\item shape and color of states indicate that they are visual shorthands
\item imposes certain constraints on states that involve I/O:
 they must be classified as Insert, Delete, or Match
 (determining what kinds of I/O happens on transitions into those states)
\item imposes certain constraints on transitions into I/O states:
 their weights must be factorizable into transition and I/O components,
 e.g. suppose state $j$ is a match state and state $i$ is any other state,
 then all transitions $i \to j$ must both absorb a non-gap input character $x$
 and emit a non-gap output character $y$,
 and the transition weight must take the form $t_{ij} \times e_j(x,y)$
 where $t_{ij}$ is a component that can depend on the source and destination state
  (but not the I/O characters)
 and $e_j(x,y)$ is a component that can depend on the I/O characters and the destination state
  (but not the source state).
\item we can then associate the I/O weight function $e_j$ with match state $j$
 and the transition weight $t_{ij}$ with a single conceptual transition $i \to j$
 that summarizes all the transitions $i \stackrel{x/y}{\to} j$
 (compare \figref{fanned-emission} and \figref{condensed-emission}).
\item The function $e_j$ can be thought of as a conditional-probability substitution matrix
 (for match states, c.f. $Q$ in \figref{condensed-emission}),
a row vector representing probability distribution
 (for insert states, c.f. $U$ in \figref{condensed-emission}),
or a column vector of conditional probabilities
 (for delete states, c.f. $V$ in \figref{condensed-emission})
\end{itemize}

\figref{legend} shows the visual notation we use in this tutorial for Moore-form transducer state types.
\widepdffig{legend}{Transducer state types.
For most Figures in the remainder of this manuscript, we will leave out the blue ``x/y'' labels on transitions,
as they are implied by the state type of the destination state.
}

Types of state: Start, Match, Insert, Delete, End, Wait, Null

Frequently abbreviated to $S,M,I,D,E,W,N$

Note the Wait states.
In our ``Moore-normal form'' for transducers, we require that all input states (Match, Delete)
are immediately preceded in the transition graph by these Wait states.
This is useful e.g. for co-ordinating multiple transducers connected together, as we shall soon see...

\figref{transitions} shows the allowed types of transition in Moore-normal form transducers.
\pngfig{transitions}{Allowed transitions...
For most Figures in the remainder of this manuscript, we will leave out the blue ``x/y'' labels on transitions,
as they are implied by the state type of the destination state.
}


\subsubsection{Moore-machine generators and recognizers}

\figref{moore-mf-generator} uses our Moore-machine visual representation
to depict the generator in \figref{mf-generator}.

\pdffig{moore-mf-generator}{Moore-normal form generator for protein sequence MF.
The states are labeled $S$ (Start), $E$ (End),
$\imath_M$ and $\imath_F$ (Insert states that emit the respective amino acid symbols),
and $W_F$ (a Wait state that pauses after emitting the final amino acid;
this is a requirement imposed by our Moore normal form).
The state labeled $\imath_Z$ (for $Z \in \{M,F\}$) has I/O function $\delta(y=Z)$.}

A few further examples of exact generators and recognizers, useful for the examples:
\begin{itemize}
\item \figref{liv-labeled} is a Moore-form recognizer for sequence LIV.
The state labeled $\delta_Z$  (for $Z \in \{L,I,V\}$) has I/O function $\delta(x=Z)$,
defined to be 1 if $x=Z$, 0 otherwise.
\item \figref{mf-labeled} is the Moore-machine recognizer for MF,
the same sequence whose generator is shown in \figref{moore-mf-generator}.
\item \figref{cs-labeled} is the Moore-machine recognizer for sequence CS.
\item \figref{null-model} is a ``null model'' generator that emits a single IID sequence
with given geometric length distribution and residue frequencies
\end{itemize}

\tallpdffig{liv-labeled}{Moore-normal form recognizer for protein sequence LIV.
The states are labeled $S$ (Start), $E$ (End),
$\delta_L$, $\delta_I$ and $\delta_V$ (Delete states that recognize the respective amino acid symbols),
$W_L$, $W_I$ and $W_V$ (Wait states that pause after recognizing each amino acid;
these are requirements imposed by our Moore normal form).
The states have been grouped (enclosed by a rectangle) to show four clusters:
 states that are visited before any of the sequence has been recognized,
 states that are visited after ``L'' has been recognized,
 states that are visited after ``I'' has been recognized,
and
 states that are visited after ``V'' has been recognized.
The I/O function associated with each Delete state $\delta_Z$ is $\delta(x=Z)$.}
\pdffig{mf-labeled}{Moore-normal form recognizer for protein sequence MF.
The states are labeled $S$ (Start), $E$ (End),
$\delta_M$ and $\delta_F$ (Delete states that recognize the respective amino acid symbols),
$W_M$ and $W_F$ (Wait states that pause after recognizing each amino acid;
these are requirements imposed by our Moore normal form).
The states have been grouped (enclosed by a rectangle) to show four clusters:
 states that are visited before any of the sequence has been recognized,
 states that are visited after ``M'' has been recognized,
and
 states that are visited after ``F'' has been recognized.
The I/O function associated with each Delete state $\delta_Z$ is $\delta(x=Z)$.}
\pdffig{cs-labeled}{Moore-normal form recognizer for protein sequence CS.
The states are labeled $S$ (Start), $E$ (End),
$\delta_C$ and $\delta_S$ (Delete states that recognize the respective amino acid symbols),
$W_C$ and $W_S$ (Wait states that pause after recognizing each amino acid;
these are requirements imposed by our Moore normal form).
The states have been grouped (enclosed by a rectangle) to show four clusters:
 states that are visited before any of the sequence has been recognized,
 states that are visited after ``C'' has been recognized,
and
 states that are visited after ``S'' has been recognized.
The I/O function associated with each Delete state $\delta_Z$ is $\delta(x=Z)$.}
\pdffig{null-model}{A simple null-model generator with geometric length parameter $p$ and residue frequency distribution $\pi$.}

\subsubsection{Substitution and identity}

\figref{substituter} shows how the Moore-normal notation is used to represent a substitution matrix...
\pdffig{substituter}{A transducer that introduces substitutions (matrix $Q$) but no indels.}

Note that the input and output sequences are always the same length...

\figref{identity} shows the special case of the identity transducer (input = output) ...
\pdffig{identity}{The identity transducer...}


\subsubsection{TKF91}

We use the TKF91 model as an example, not because it is the best model of indels
 (it has deficiencies, most notably the linear gap penalty);
rather, because it is canonical, widely-known, and illustrative of the general properties of transducers...

\pdffig{tkf91}{...}

Definitions of probabilities $a,b,c...$ and parameters $\lambda,\mu,R,t$

\pdffig{tkf91-labeled}{
The TKF91 transducer.
So that we can later refer to the states by name,
rather than writing the I/O weight functions directly on each state
we have instead written a state label
 $S,M,I,D,E,W$ (Start, Match, Insert, Delete, End, Wait).
It so happens that the TKF91 transducer has one of each of these kinds of state.
For each of the I/O states ($I$, $D$ and $M$) we must, of course, still specify an I/O weight function.
So,
 $\exp(Rt)$ is the substitution matrix for the $M$ (Match) state,
 $\pi$ is the vector of weights
  corresponding to the probability distribution of inserted characters for the $I$ (Insert) state,
 and
 $(1,1,\ldots,1)$
 is the vector of weights corresponding to
 the conditional probabilities that any given character will be deleted by the $D$ (Delete) state
 (in the TKF91 model, deletion rate is independent of the actual character being deleted, so these are all 1).
}

\figref{tkf91-labeled} is a version of \figref{tkf91}
where, rather than writing the I/O weight functions directly on each state (as in \figref{tkf91}),
we have instead written a state label (as in \figref{liv-labeled}, \figref{mf-labeled} and \figref{cs-labeled}).
The state labels are $S,M,I,D,E,W$ (interpretation: Start, Match, Insert, Delete, End, Wait).

\pdffig{tkf91-root}{The equilibrium distribution for the TKF91 model is essentially the same generator as \figref{null} but with $p=\lambda/\mu$.}

Equilibrium (root) generator for TKF91...


\subsection{Composition}

There are various ways of connecting transducers by constraining some of their tapes to be the same.
The first of these that we will consider is transducer ``composition''.

Composition is when you feed the output of one transducer, $T$, into the input of another, $U$...
the two transducers are now connected in a sense (outputs of $T$ can be synchronized with inputs from $U$)
and we can construct a single, monster transducer that simulates the connected ensemble of $T$ followed by $U$...

From two transducers $T$ and $U$,
we make a new transducer $TU$
wherein every state corresponds to a pair $(t,u)$ of $T$- and $U$-states...

Composition is like matrix multiplication...
when we sum over paths through $TU$,
we are summing over the intermediate sequence
(the output of $T$, which is the input of $U$)...

\subsubsection{Multiplying two substitution models}

\pdffig{substituter2}{A transducer that introduces substitutions (matrix $R$) but no indels. Compare to \figref{substituter}, which is the same model but with substitution matrix $Q$ instead of $R$.}

\pdffig{substituter-substituter2}{Composition of \figref{substituter} and \figref{substituter2}.}

Like two consecutive branches $x \stackrel{Q}{\to} y \stackrel{R}{\to} z$...
each branch is modeled by a different substitution matrix, $Q$ and $R$

The I/O function for the composite Match state is the matrix multiplication, $QR$...
that is,
if $x$ denotes the input symbol to the two-transducer ensemble (input into the Q-substituter),
$y$ denotes the output symbol from the two-transducer ensemble (output from the R-substituter),
and $z$ denotes the unobserved intermediate symbol (output from Q and input to R),
then the I/O weight function for the composite state QR in the two-transducer ensemble is
\[
(QR)_{xy} = \sum_z Q_{xz} R_{zy}
\]

\subsubsection{Multiplying TKF91 with itself}

\widepdffig{tkf91-tkf91}{Composition of TKF91 model (\figref{tkf91-labeled}) with itself.}

Again like two consecutive branches $x \to y \to z$
but now TKF91 is acting along each branch...

This is an ensemble of two transducers.
Input sequence is fed into first TKF91;
output of this first TKF91 transducer is an intermediate sequence that is fed into the input of the second TKF91;
output of this second TKF91 is the output of the entire ensemble...
When summing over paths, we are effectively summing over the intermediate sequence...

Since it is an ensemble of two transducers, every state corresponds to a tuple $(b_1,b_2)$
where
$b_1$ is the state of the first TKF91 transducer and
$b_2$ is the state of the second TKF91 transducer.
The meaning of the various states in this model are
\begin{itemize}
\item $SS$ ...
\item $EE$ ...
\item $WW$ ...
\item $SI$ ...
\item $II$ ...
\item $MI$ ...
\item $IM$ ...
\item $MM$ ...
\item $ID$ ...
\item $MD$ ...
\item $DW$ ...
\end{itemize}

In a specific sense (summing over paths)
this composite transducer is equivalent to the single transducer in \figref{tkf91}
with the time parameter double ($t \to 2t$)...
this statement is equivalent to a form of the Chapman-Kolmogorov equation,
$B(t)B(t) \equiv B(2t)$

In fact TKF91 is currently the only nontrivial indel model known to have this property
(by ``nontrivial'' we mean excluding substitution-only models such as \figref{substituter},
which are essentially special cases of TKF91).
An open question is whether there are any such transducers for affine-gap versions of TKF91...
we exclude TKF92 from this since it does not technically operate on strings, but rather
sequences of strings (fragments) with immovable boundaries...

\subsection{Removal of null states}

In \figref{tkf91-tkf91}, the state $ID$ is null:
it corresponds to an insertion by the first TKF91 transducer
that is then immediately deleted by the second TKF91 transducer,
so there is no net insertion or deletion.

It is sometimes useful to remove these states,
i.e. find an equivalent transducer that lacks them
(e.g. if trying to transform a transducer into strict Moore-normal form)...


\subsubsection{Composition of MF-generator with substituter}
\needfig{mf-substituter}

\needfigref{mf-substituter} is the composition of MF-generator with TKF91...
Note that this is quite similar to a probabilistic weight matrix trained on a single sequence...

Since it is an ensemble of two transducers, every state corresponds to a tuple $(i,b)$
where
$i$ is the state of the generator transducer and
$b$ is the state of the substituter transducer.
The meaning of the various states in this model are
\begin{itemize}
\item List of states...
\end{itemize}

\subsubsection{Composition of MF-generator with TKF91}
\tallpdffig{mf-tkf91}{Composition of MF-generator (\figref{moore-mf-generator}) with TKF91 transducer (\figref{tkf91-labeled})}

\needfigref{mf-tkf91} is the composition of MF-generator with TKF91...

Since it is an ensemble of two transducers, every state corresponds to a tuple $(i,b)$
where
$i$ is the state of the generator transducer and
$b$ is the state of the TKF91 transducer.
The meaning of the various states in this model are
\begin{itemize}
\item List of states...
\end{itemize}

Note that the only state types are Start, End, Insert, and Null;
there are no Match or Delete states, even though these states occur in the TKF91 transducer.
The reason is that, even when the TKF91 is in a state that accepts input symbols (Match or Delete),
the input symbol was inserted by the MF-generator;
the MF-generator does not itself accept any input,
so the entire ensemble accepts no input
(and is therefore a generator).

Note that this is quite similar to a profile HMM trained on a single sequence...

\subsubsection{Composition of TKF91 with LIV-recognizer}

\tallpdffig{tkf91-liv}{...}

Since it is an ensemble of two transducers, every state corresponds to a tuple $(b,d)$
where
$b$ is the state of the TKF91 transducer and
$d$ is the state of the recognizer transducer.
The meaning of the various states in this model are
\begin{itemize}
\item List of states...
\end{itemize}

Note that there are only delete states in this (as in LIV-recognizer)...
explain why this arises (because LIV-recognizer deletes anything TKF91 emits)
and the meaning (it is a probabilistic recognition profile for ancestral descendants of sequence LIV)...

\subsubsection{Composition of TKF91 with MF-recognizer}

\tallpdffig{tkf91-mf}{...}

\subsubsection{Composition of MF-generator, substituter and CS-recognizer}

An alternative way of computing the weight, that expands the model to include the data...
\needfig{mf-substituter-cs}

Note that we could not e.g. compose MF-generator$\to$substituter$\to$LIV-recognizer
(or rather we could, but since substituter does not change sequence length,
there would be no valid path, and probability would come out as zero,
which is the correctly computed probability of transforming MF to LIV by point substitutions alone)


\subsubsection{Composition of MF-generator, TKF91 and LIV-recognizer}

\widepdffig{mf-tkf91-liv}{...}

In this diagram, transitions are unlabeled to avoid clutter...

Since it is an ensemble of three transducers, every state corresponds to a tuple $(i,b,d)$
where
$i$ is the state of the generator transducer,
$b$ is the state of the TKF91 transducer and
$d$ is the state of the recognizer transducer.
The meaning of the various states in this model are
\begin{itemize}
\item List of states...
\end{itemize}

Note that there are no match/insert/delete states --- only null states...
thus this is more like a straightforward Markov model...
in fact Markov models are a special case of an input/output machine where the input and output are both null
(just as an HMM can be considered as a special case where the input is null, i.e. a generator,
and a parser or recognizer is a special case where the output is null)...

Probability $P(Y=\mbox{LIV}|X=\mbox{MF})$ for TKF91 model
is equal to sum of all path weights from start to end in this Markov model...

Note how the structure of the Markov model is directly analogous to a dynamic programming matrix...
rows, columns, cells...

We could not do this with substituter instead of TKF91
(or rather we could, but substituter does not change sequence length,
so there would be no valid path, and probability would correctly come out as zero)

\subsection{Fork}

The second operation for connecting transducers that we consider
 is one that we call ``fork''.
It is also commonly called the ``intersection'' of two transducers
 (e.g. that is the name used by Wikipedia).

Fork is when you feed the same input tape into two transducers in parallel...

As with a transducer composition,
a fork is constructed by taking the Cartesian product of two transducers' state spaces.
From two transducers $T$ and $U$,
we make a new transducer $TU$
wherein every state corresponds to a pair $(t,u)$ of $T$- and $U$-states...

\subsubsection{Compositions, forks and Felsenstein's algorithm}
\seclabel{Felsenstein}

If a composition is like matrix multiplication,
i.e. the operation of evolution along a contiguous branch of the phylogenetic tree,
then a fork is like a bifurcation at a node in the phylogenetic tree,
where a branch splits into two child branches.

It corresponds to the pointwise multiplication step in the Felsenstein pruning algorithm ---
i.e. the calculation
\[
P(\mbox{descendants}|\mbox{parent}) =
P(\mbox{left child and its descendants}|\mbox{parent})
P(\mbox{right child and its descendants}|\mbox{parent})
\]

Specifically, in the Felsenstein algorithm,
we define $G^{(n)}(x)$ to be the probability of all observed descendants of node $n$,
conditional on node $n$ having been in state $x$.
Let us further suppose that $M^{(n)}$ is the conditional substitution matrix
for the branch above node $n$ (coming from $n$'s parent), so
\[
M^{(n)}_{ij}=P(\mbox{node $n$ is in state $j$}|\mbox{parent of node $n$ is in state $i$})
\]
Then we can write the core recursion of Felsenstein's pruning algorithm in matrix form;
$G^{(n)}$ is a vector, $M^{(n)}$ is a matrix, and the core recursion is
\[
G^{(n)} = \left( M^{(l)} \times G^{(l)} \right) \oplus \left( M^{(r)} \times G^{(r)} \right)
\]
where $(l,r)$ are the left- and right-children of node $n$,
$\times$ denotes matrix multiplication,
and $\oplus$ denotes the vector {\em pointwise product} (also called the {\em Hadamard product}),
defined as follows for two vectors $A$ and $B$:
\[
(A \oplus B)_i = A_i B_i,
\quad \quad \quad
A \oplus B = \left( \begin{array}{c}
A_1 B_1 \\ A_2 B_2 \\ A_3 B_3 \\ \ldots \\ A_K B_K
\end{array} \right)
\]

Thus the two core steps of Felsenstein's algorithm (in matrix notation)
are (a) matrix multiplication and (b) the pointwise product.
We have defined the transducer equivalent of matrix multiplication,
now we need to define the transducer equivalent of the vector pointwise product.

Note also that $G^{(n)}(x)$ is the probability of node $n$'s observed descendants
{\em conditional on} $x$, the state of node $n$.
Thus $G^{(n)}$ is similar to a recognition profile,
where the computed weight for a sequence $S$ represents
the probability of some event (recognition) conditional on having $S$ as input,
i.e. a probability of the form $P(\ldots|S)$
(as opposed to a probability distribution of the form $P(S|\ldots)$ where the sequence $S$ is the output,
as is computed by generative profiles).

Finally consider the initialization step of the Felsenstein algorithm.
Let $n$ be a leaf node and $y$ the observed character at that node.
The initialization step is
\[
G^{(n)}(x) = \delta(x=y)
\]

\subsubsection{Fork of substituter-CS with substituter-MF}
\seclabel{fork-subcs-submf}

\needfig{fork-subcs-submf}

Note that this transducer is a recognizer (all delete states).
Note also that it computes $G^{(n)}$ from \secref{Felsenstein}...

This is an ensemble of four transducers.
Input sequence is duplicated;
one copy fed into substituter (\figref{substituter}),
output fed into exact-matcher for CS (\figref{cs-labeled});
other copy fed into a separate substituter (\figref{substituter2}),
output fed into exact-matcher for MF (\figref{mf-labeled})...

Since it is an ensemble of four transducers, every state corresponds to a tuple $(b_1,d_1,b_2,d_2)$
where
$b_1$ is the state of the substituter transducer on the CS-branch (\figref{substituter}),
$d_1$ is the state of the exact-matcher for sequence CS (\figref{cs-labeled}),
$b_2$ is the state of the substituter transducer on the MF-branch (\figref{substituter2}),
$d_1$ is the state of the exact-matcher for sequence MF (\figref{mf-labeled})...

In general we can write the I/O label for such a state as $(b_1 d_1) \oplus (b_2 d_2)$.
For example consider the case where $(b_1,d_1,b_2,d_2) = (Q,\delta_C,R,\delta_M)$.
Then $(Q,R)$ denote substitution matrices
and $(\delta_C,\delta_M)$ denote vectors,
so the label $(Q \delta_C) \oplus (R \delta_M)$
denotes the vector whose elements given by
\[
\left( (Q \delta_C) \oplus (R \delta_M) \right)_x
= Q_{xC} R_{xM}
\]
i.e. the $x$'th entry in this vector denotes the probability that an input symbol $x$
would mutate to C on the $Q$-branch and M on the $R$-branch.

\subsubsection{Composition of simple prior with fork of substituter-MF and substituter-CS}

\needfig{root-fork-submf-subcs}

Forward sum computes final Felsenstein probability for this two-branch tree...

Note that this transducer is a pure Markov chain (no input/output states).
Note also that it computes the final Felsenstein probability.

Sample traceback path yields sample from posterior distribution over ancestral sequences...

\subsubsection{Fork of TKF91-LIV with TKF91-MF}

\widepdffig{fork-tkf91liv-tkf91mf}{ Autogenerated ...}

\figref{fork-tkf91liv-tkf91mf} is the recognition profile for the common ancestor of LIV and MF...

This is an ensemble of four transducers.
Input sequence is duplicated;
one copy fed into TKF91 (\figref{tkf91-labeled}),
output fed into exact-matcher for LIV (\figref{liv-labeled});
other copy fed into a separate TKF91 machine (\figref{tkf91-labeled}),
output fed into exact-matcher for MF (\figref{mf-labeled})...

Since it is an ensemble of four transducers, every state corresponds to a tuple $(b_1,d_1,b_2,d_2)$
where
$b_1$ is the state of the TKF91 transducer on the LIV-branch (\figref{tkf91-labeled}),
$d_1$ is the state of the exact-matcher for sequence LIV (\figref{liv-labeled}),
$b_2$ is the state of the TKF91 transducer on the MF-branch (\figref{tkf91-labeled}),
$d_1$ is the state of the exact-matcher for sequence MF (\figref{mf-labeled})...

In general we can label such a state as $(b_1 d_1) \oplus (b_2 d_2)$,
as described in \figref{fork-subcs-submf}

Note that, as with \figref{mf-tkf91-liv},
underlying structure is somewhat like a DP matrix,
with rows, columns and cells.
In fact, modulo some quirks of the automatic graph layout (performed by graphviz's 'dot' program),
\figref{fork-tkf91liv-tkf91mf} and \figref{mf-tkf91-liv} are structurally quite similar.
However, compared to \figref{mf-tkf91-liv},
\figref{fork-tkf91liv-tkf91mf} has more states in each ``cell'',
because this transducer tracks two separate branches
(whereas \figref{mf-tkf91-liv} only tracks one branch).

Note that, since this is a recognizer, it has delete states but no match/insert states...
the delete states do not necessarily correspond to deletions by the TKF91 transducers
(all symbols are ultimately deleted by the exact-match recognizers for LIV and MF,
so even if the two TKF91 transducers allow symbols to pass through undeleted,
they will still be deleted by the exact-matchers).

In fact, this transducer's states distinguish between
deletion events on one branch {\em vs} insertion events on the other:
this is significant because a ``deleted'' residue is homologous to a residue in the ancestral sequence,
while an ``inserted'' residue is not.
There are, in fact, four delete states in each ``cell'' of the matrix,
corresponding to four fates of the input symbol after it is duplicated:
(a) both copies of input symbol pass successfully through respective TKF91 transducers
and are then deleted by respective downstream exact-matchers for sequences LIV and MF
 (e.g. $M D_L \oplus M D_F$);
(b) one copy of input symbol deleted by TKF91 transducer on LIV-branch,
leaving downstream LIV-matcher idling in a wait state;
other copy passes through TKF91 transducer on MF-branch and is then deleted by downstream MF-matcher
 (e.g. $D W_0 \oplus M D_F$);
(c) one copy of input symbol passes through TKF91 transducer on LIV-branch
and is then deleted by downstream LIV-matcher;
other copy is deleted by TKF91 transducer on MF-branch,
leaving downstream MF-matcher idling in a wait state;
 (e.g. $M D_L \oplus D W_0$);
(d) both input symbols deleted by respective TKF91 transducers,
while downstream exact-matchers idle in wait states without seeing any input
 (e.g. $D W_0 \oplus D W_0$).

The other states in each cell of the matrix are insertion states
(where the symbols in sequences LIV or MF are accounted for by insertions from the TKF91 transducers,
rather than by input symbols)
and wait states (where the recognizer waits for the next input symbol).



\subsubsection{Composition of simple prior with fork of TKF91-MF and TKF91-LIV}

\needfig{root-fork-tkf91mf-tkf91liv}

Forward sum computes final Felsenstein probability...

Sample traceback path yields sample from posterior distribution over ancestral sequences...

\subsubsection{Best reconstruction of ancestor to MF and LIV}

\needfig{viterbi-root-fork-tkf91mf-tkf91liv}

Highlighted Viterbi traceback path...

e.g.
\begin{tabular}{ccc}
L & I & V \\
M & F & -
\end{tabular}

\needfig{viterbi-fork-tkf91mf-tkf91liv}

Relabeling (removing the root generator) we now have a reconstructed linear profile
for the sequence at the ancestral node,
as might be computed by progressive alignment...

Note that this is a recognition profile (all delete states)...

\subsubsection{Sampling reconstructions of ancestor to MF and LIV}

\needfig{forward2-root-fork-tkf91mf-tkf91liv}

Instead of just doing Viterbi, we can sample more than one traceback path...

Now as well as this path
\begin{tabular}{ccc}
L & I & V \\
M & F & -
\end{tabular}

we also have this path
\begin{tabular}{ccc}
L & I & V \\
- & M & F
\end{tabular}

\needfig{forward2-fork-tkf91mf-tkf91liv}

Relabeling again, we still have a recognition profile, but it is now branched,
reflecting the possible uncertainty in our alignment...

Note that these are all still approximations to the full
recognition profile for the ancestor, which is \figref{fork-tkf91liv-tkf91mf}

\subsubsection{Fork of TKF91-CS with (fork of TKF91-LIV and TKF91-MF)}

\needfig{fork-tkf91cs-fork-tkf91mf-tkf91liv
 Fork of TKF91-CS with (fork of TKF91-LIV and TKF91-MF)}

\subsubsection{Felsenstein probability for (CS,(LIV,MF))}

\needfig{root-fork-tkf91cs-fork-tkf91mf-tkf91liv
 Composition of prior with (fork of TKF91-CS with (fork of TKF91-LIV and TKF91-MF))}

We have now seen the individual steps of the transducer version of the Felsenstein recursion...

The full recursion (with the same $O(L^N)$ complexity as the Sankoff algorithm
for simultaneously aligning $N$ sequences of length $L$, ignoring secondary structure)
involves starting with exact-match recognition profiles at the leaves (\figref{mf-labeled}, \figref{liv-labeled}),
using those to construct recognition profiles for the parents (\figref{fork-tkf91liv-tkf91mf}),
and progressively climbing the tree toward the root,
constructing ancestral recognition profiles as you go.
At the root, you compose the root generator with the root recognition profile,
and compute the Forward probability.

\subsubsection{Progressive alignment version of Felsenstein recursion}

The ``progressive alignment'' version,
equivalent to doing Felsenstein's pruning recursion on a single alignment
found using the progressive alignment algorithm,
involves sampling the single best linear recognition profile of the parent,
as in \figref{fork-tkf91mf-tkf91liv-viterbi-traceback}...

\needfig{fork-tkf91cs-viterbi-fork-tkf91mf-tkf91liv}

This can be recognized as a form of ``sequence-profile'' alignment as familiar from progressive alignment,
except that we don't really make a distinction between a sequence and a profile
(in that we convert sequences into recognition profiles anyway).

\needfig{root-fork-tkf91cs-viterbi-fork-tkf91mf-tkf91liv}

\needfigref{root-fork-tkf91cs-viterbi-fork-tkf91mf-tkf91liv} introduces the prior at the root...

\needfig{viterbi-root-fork-tkf91cs-viterbi-fork-tkf91mf-tkf91liv}

\needfigref{viterbi-root-fork-tkf91cs-viterbi-fork-tkf91mf-tkf91liv} is the recognition profile for the root node...

\subsection{Stochastic lower bound version of Felsenstein recursion}

Our stochastic lower bound version is intermediate to these two.
Rather than just sampling the best linear profile for each parent
(as in \figref{fork-tkf91mf-tkf91liv-viterbi-traceback}),
we sample some fixed number of such paths
(as in \figref{fork-tkf91mf-tkf91liv-forward-tracebacks}).
This allows us to sum over some amount of alignment uncertainty,
while avoiding the full complexity of the complete ancestral profile
(\figref{fork-tkf91mf-tkf91liv-viterbi-traceback})...

By sampling a fixed number of traceback paths,
we can construct a recognition profile for the ancestral sequence
that is linearly bounded in size and offers a stochastic ``lower bound''
on the probability computed by the full Felsenstein transducer in \needfigref{root-fork-tkf91mf-tkf91liv}.

\needfig{fork-tkf91cs-forward2-fork-tkf91mf-tkf91liv}

Again this is a ``sequence-profile'' alignment.
Unlike in progressive alignment
 (but quite like e.g. partial order alignment \needcite{Grasso, Lee {\em et al}}),
one of the profiles is now branched
 (because we sampled more than one path to construct it in \needfigref{forward2-fork-tkf91mf-tkf91liv})...

\needfig{root-fork-tkf91cs-forward2-fork-tkf91mf-tkf91liv}

\needfigref{root-fork-tkf91cs-forward2-fork-tkf91mf-tkf91liv}
introduces the prior at the root, and represents the final Felsenstein probability.

If there were more levels of the tree above this, we would keep on sampling...

\end{document}
